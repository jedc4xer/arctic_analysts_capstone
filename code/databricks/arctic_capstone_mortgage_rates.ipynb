{"cells":[{"cell_type":"markdown","source":["## Mortgage Rates Scraper\n**Intended Utility**\n>This databrick is designed to be utilized as part of a pipeline.<br><br>\n>The data can be scraped once a month to check for any updates. The databrick will check<br>\n>the database and if the last data is older than 1 month, then it will scrape again. If the <br>\n>data is not older than 1 month, then the databrick will take no action.<br><br>\n>If a scrape is conducted, then the databrick will clean and transform the data before sending it<br>\n>to a storage container and then updating the database.\n\n**Helpful Links**\n>https://dc.urbanturf.com/articles/blog/first-timer_primer_interest_rates_and_mortgage_points/6745"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8baf503-580f-43ef-9958-849efe84976a"}}},{"cell_type":"markdown","source":["**Configuration:**\n> These cells are responsible for configuring the primary aspects of the databrick.<br>\n\n**Config Part 1:** Import Libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4eb6d759-39e5-473e-b86b-c69a702e0227"}}},{"cell_type":"code","source":["import io\nimport requests\nimport datetime as dt\nimport pyspark.pandas as ps\nfrom typing import Iterable \nfrom bs4 import BeautifulSoup\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.functions import split\nfrom pyspark.sql.functions import array, col, explode, lit, struct, when\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d9e55c1-1a92-48fd-8fe7-20d092532840"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Config Part 2:** Create I/O widgets for pipeline."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a98a8c9-9b08-47af-873f-8d4589cf6ff9"}}},{"cell_type":"code","source":["# These widgets allow the pipeline to set the file I/O information.\n\ndbutils.widgets.text(\"input\", \"\",\"\") \ndbutils.widgets.get(\"input\")\n \ndbutils.widgets.text(\"output\", \"\",\"\") \ndbutils.widgets.get(\"output\")\n \ndbutils.widgets.text(\"filename\", \"\",\"\") \ndbutils.widgets.get(\"filename\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f32b74f9-4f83-4d3e-a3e0-329b01cee4e9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[146]: &#39;mortgage_data_from_databrick&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[146]: &#39;mortgage_data_from_databrick&#39;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Config Part 3:** Designate Mount Points"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c12ae54-ba0a-4c06-acae-2380404f2f3b"}}},{"cell_type":"code","source":["def mount_storage(mount_goal):\n    storageAccount = mount_goal['account']\n    storageContainer = mount_goal['container']\n    clientSecret = \"B4g8Q~1VyZJa5WszLHwdEQNq4YIaHmT4DevRBcwI\"\n    clientid = \"2ca50102-5717-4373-b796-39d06568588d\"\n    mount_point = mount_goal['mount']\n\n    configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n           \"fs.azure.account.oauth2.client.id\": clientid,\n           \"fs.azure.account.oauth2.client.secret\": clientSecret,\n           \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/d46b54b2-a652-420b-aa5a-2ef7f8fc706e/oauth2/token\",\n           \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\n\n    try: \n        dbutils.fs.unmount(mount_point)\n    except:\n        pass\n\n    dbutils.fs.mount(\n    source = \"abfss://\"+storageContainer+\"@\"+storageAccount+\".dfs.core.windows.net/\",\n    mount_point = mount_point,\n    extra_configs = configs)\n    \n    return mount_point\n    \nin_path = getArgument(\"input\").split(\"/\")\nout_path = getArgument(\"output\").split(\"/\")\n\nstorage_info = {\n    'read': {'account': in_path[0], 'container': in_path[1], 'mount': \"/mnt/arctic_analysts_mortgage_info_scraper_read\"},\n    'write': {'account': out_path[0], 'container': out_path[1], 'mount': \"/mnt/arctic_analysts_mortgage_info_scraper_write\"}\n}\n\ntry:\n    read_path = mount_storage(storage_info['read'])\n    write_path = mount_storage(storage_info['write'])\n        \nexcept Exception as E:\n    print(E[:-50])\n    \nprint(f'Read Path: {read_path}\\nWrite Path: {write_path}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60799d54-d722-4fbc-8117-a6e21c8c52f1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/mnt/arctic_analysts_mortgage_info_scraper_read has been unmounted.\n/mnt/arctic_analysts_mortgage_info_scraper_write has been unmounted.\nRead Path: /mnt/arctic_analysts_mortgage_info_scraper_read\nWrite Path: /mnt/arctic_analysts_mortgage_info_scraper_write\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/arctic_analysts_mortgage_info_scraper_read has been unmounted.\n/mnt/arctic_analysts_mortgage_info_scraper_write has been unmounted.\nRead Path: /mnt/arctic_analysts_mortgage_info_scraper_read\nWrite Path: /mnt/arctic_analysts_mortgage_info_scraper_write\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Database Data Check**\n>These cells check if there is any new data.<br>\n\n**Data Check Part 1:** Check the most recent data in the data lake."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da76c2a9-516f-4c76-b948-d882830194c7"}}},{"cell_type":"code","source":["# Check if we already have mortgage rate data. \ntry:\n    filename = getArgument(\"filename\")\n    mortgage_rates = spark.read.json(read_path + '/' + filename + '.json')\n    file_found = True\nexcept Exception as E:\n    print(E)\n    print('There does not appear to be a file associated with this dataset.')\n    print('A new file will be created.')\n    file_found = False\n    \n# Check if enough days have elapsed to justify another scrape.\nif file_found:\n    # Get todays date\n    todays_date = dt.datetime.today().date()\n    calculate_date = udf(lambda x: dt.datetime.strftime(dt.datetime.strptime(x[1]+ \"-\" + x[0] + '-' + '01', '%Y-%B-%d'), '%Y-%m-%d'))\n    \n    # Get the last date in the current data.\n    computed_date = mortgage_rates.where(col(\"Month\") != 'ANNUAL AVERAGE').withColumn(\"Date\", calculate_date(array('Month','Year')))\n    computed_date = computed_date.filter(computed_date.AveragePoints.isNotNull())\n    most_recent_date = computed_date.sort(computed_date.Date.desc()).select(\"Date\").limit(1).collect()[0].asDict()['Date']\n    most_recent_date = dt.datetime.strptime(most_recent_date, '%Y-%m-%d').date()\n    \n    # Check if more than 60 days have elapsed since the last data.\n    days_since = (todays_date - most_recent_date).days\n    if days_since > 60:\n        \n        # This is set to 60 temporarily, but might be dropped to 45\n        print('More than 60 days have elapsed since the last scrape.')\n        print('A new scrape will be run.')\n        scrape_permitted = True\n    else:\n        print(f'Not enough time has elapsed. You must wait for {60 - (days_since)} more days to scrape again.')\n        scrape_permitted = False\nelse:\n    scrape_permitted = True"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbf2ca4d-7281-42d3-88e7-44af74143bf1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Not enough time has elapsed. You must wait for 13 more days to scrape again.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Not enough time has elapsed. You must wait for 13 more days to scrape again.\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Data Scraper\n> These cells are responsible for controlling the scrape behavior.<br>\n\n**Scrape Part 1:**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"513d7906-db21-4843-b5f2-3586a7811e05"}}},{"cell_type":"code","source":["# Establish a list of months for conversions.\nmonth_list = [\n        'JANUARY',\n        'FEBRUARY',\n        'MARCH',\n        'APRIL',\n        'MAY',\n        'JUNE',\n        'JULY',\n        'AUGUST',\n        'SEPTEMBER',\n        'OCTOBER',\n        'NOVEMBER',\n        'DECEMBER',\n        'ANNUAL AVERAGE'\n    ]\n\ndef get_header(table):\n    \"\"\" This function returns the header row of the scraped table. \"\"\"\n    row_values = []\n\n    row = table.find('tr').get_text(\"|\").split(\"|\")\n    row_values.append(row)\n    new_header = []\n    for i, item in enumerate(row_values[0]):\n        if ('2' in item or '1' in item):\n            new_header.append(item + '_Rate')\n            new_header.append(item + '_Pts')\n    return new_header\n\ndef get_rows(table):\n    cells = table.find_all('tr')\n    \n    table_rows = []\n    for item in cells:\n        new_row = item.text.upper().strip().replace('\\n',\"|\").replace('\\xa0',\"\").split(\"|\")\n        if new_row[0] in month_list:\n            table_rows.append(new_row)\n    return table_rows\n\ndef melt(\n    df: DataFrame,\n    id_vars: Iterable[str], value_vars: Iterable[str],\n    var_name: str=\"YearAttribute\",\n    value_name: str=\"MortgageRateValue\"):\n    \n    _vars_and_vals = array(*(\n        struct(lit(c).alias(var_name), col(c).alias(value_name))\n        for c in value_vars))\n    \n    _tmp = df.withColumn(\"_vars_and_vals\", explode(_vars_and_vals))\n    \n    cols = id_vars + [\n        col(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n    \n    return _tmp.select(*cols)\n\ndef scrape_mortgage_rates():\n    url = 'https://www.freddiemac.com/pmms/pmms30'\n    \n    # Get the data from the website\n    raw_data = requests.get(url).text\n    \n    # Create soup with BeautifulSoup\n    soup = BeautifulSoup(raw_data)\n    \n    # Get all tables on the page\n    tables = soup.find_all('table')\n    \n    # Create a list for dataframes\n    dataframes = []\n    \n    # For every table in the available tables, (except for the last few)\n    for table_count, table in enumerate(tables[:-2]):\n        \n        # Get the header of the table\n        new_header = get_header(table)\n        \n        # Add a month column to the header that is named in a way that we can drop duplicates.\n        if table_count == 0:\n            new_header.insert(0, 'Primary_Month')\n        else:\n            new_header.insert(0, 'Month')\n\n        # Get all the rows of the table\n        table_rows = get_rows(table)\n\n        # Create a dataframe from the rows and set header as column names\n        df = spark.createDataFrame(table_rows, new_header)\n        \n        # Add the new dataframe to the list of dataframes\n        dataframes.append(df)    \n\n    # Combine all dataframes to large dataframe\n    for i, dataframe in enumerate(dataframes):\n        if i == 0:\n            final_frame = dataframe\n        else:\n            final_frame = final_frame.join(dataframe, final_frame[\"Primary_Month\"] == dataframe[\"Month\"], \"outer\")\n            final_frame = final_frame.drop('Month')\n            \n    # Convert from wide form to long form.\n    melted_frame = melt(final_frame, id_vars = ['Primary_Month'], value_vars = [_ for _ in final_frame.columns if _ != 'Primary_Month'])\n\n    # Create a new column to hold the year as well as a new column to hold the attribute\n    split_df = melted_frame.withColumn('Year', split(melted_frame['YearAttribute'], '_').getItem(0)) \\\n                           .withColumn('Attribute', split(melted_frame['YearAttribute'], '_').getItem(1))\n\n    # Drop the YearAttribute column and rename the month column for continuity\n    split_df = split_df.drop(col(\"YearAttribute\"))\n    renamed_df = split_df.withColumnRenamed('Primary_Month','PrimaryMonth')\n    \n    # Replacing all the blanks in the the MortgageRateValue with None\n    final_df = renamed_df.withColumn(\"MortgageRateValue\", when(col(\"MortgageRateValue\")==\"\", None).otherwise(col(\"MortgageRateValue\")))  \n    final_df = final_df.withColumn('MortgageRateValue', when(col(\"MortgageRateValue\")==\"\", None).otherwise(col(\"MortgageRateValue\")))\n    \n    # Split the dataframe into two, one with points, and one with rate\n    attribute_frame = final_df.where(final_df.Attribute == 'Pts') \\\n                              .withColumnRenamed(\"MortgageRateValue\", \"AveragePoints\") \\\n                              .drop('Attribute')\n\n    rate_frame = final_df.where(final_df.Attribute == 'Rate') \\\n                         .withColumnRenamed(\"MortgageRateValue\", \"AverageRate\") \\\n                         .withColumnRenamed(\"Year\", 'drop_year') \\\n                         .withColumnRenamed('PrimaryMonth', 'Month') \\\n                         .drop('Attribute')\n\n    # Rejoin dataframes so that there are two new columns, rate and points, then drop the year and primary month\n    finished_df = rate_frame.join(attribute_frame, (rate_frame.drop_year == attribute_frame.Year) & (rate_frame.Month == attribute_frame.PrimaryMonth), how = 'inner')\n    finished_df = finished_df.drop('drop_year').drop('PrimaryMonth')\n\n    # Filter out the annual averages\n    finished_df = finished_df.where(col(\"Month\") != 'ANNUAL AVERAGE') \n            \n    return finished_df\n\ndef save_scrape_to_blob(df):\n    final_path = write_path + '/mortgage_data_from_databrick'\n\n    # Reduce to single file and write to blob\n    df.repartition(1).write.format(\"com.databricks.spark.json\") \\\n    .mode('overwrite').option(\"header\", \"True\") \\\n    .json(final_path)\n\n    # Locate file in blob\n    files = dbutils.fs.ls(final_path)\n    json_file = [x.path for x in files if x.path.endswith(\".json\")][0]\n\n    # Move file out of directory into main blob and delete junk files\n    dbutils.fs.mv(json_file, final_path + \".json\")\n    dbutils.fs.rm(final_path, recurse = True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff8567ab-dc62-4d27-b442-2edcadf593b3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["if scrape_permitted:        \n    finished_df = scrape_mortgage_rates()\n    save_scrape_to_blob(finished_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ab506dd-49b1-455c-a619-f57a0885d4f3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# If we want to read the data from the blob\n#mortgage_df = spark.read.options(inferSchema='True', header='True').json(read_path + \"/\" + getArgument('filename') + \".json\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5336221e-7986-4b45-9df5-ecd999794f1b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"arctic_capstone_mortgage_rates","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{"input":{"nuid":"017d309a-a512-4433-8e42-f89c52630341","currentValue":"gen10datafund2202/z-arctic-analysts-capstone-main","widgetInfo":{"widgetType":"text","name":"input","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"filename":{"nuid":"9193c51e-5816-4b6a-a61c-041339902cdf","currentValue":"mortgage_data_from_databrick","widgetInfo":{"widgetType":"text","name":"filename","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"output":{"nuid":"ebc52212-6387-4054-9528-822c123ae1e7","currentValue":"gen10datafund2202/z-arctic-capstone-cleaned-data","widgetInfo":{"widgetType":"text","name":"output","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":1967226895816169}},"nbformat":4,"nbformat_minor":0}

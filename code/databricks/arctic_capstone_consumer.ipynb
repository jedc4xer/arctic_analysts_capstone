{"cells":[{"cell_type":"markdown","source":["## Kafka Consumer\n**Intended Utility**\n>This databrick is one piece of a \"producer - consumer\" pair. <br>\n>It is intended to simulate a stream of data between a source and a datalake.<br>\n\n>In this scenario, the source is an Azure Blob, and the data is Zillow Home Price Data.<br>\n>The data that is consumed in this consumer will be saved to an Azure Blob, and then the <br>\n>the database will be periodically updated from the blob."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b033999d-97ab-4acb-abe7-d233f78d3504"}}},{"cell_type":"markdown","source":["**Configuration:**\n> These cells are responsible for configuring the primary aspects of the databrick.<br>\n\n**Config Part 1:** Import Libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b3ece6a-7ac3-43a3-ad14-eb3911f6b22b"}}},{"cell_type":"code","source":["#pip install confluent-kafka\n#shouldn't need to do this"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e107440-f8e0-45ec-beee-3e5911bc6c04"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Error Callbacks**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"423bfeb6-dfc6-44a3-841b-6b4d66525c0f"}}},{"cell_type":"code","source":["def error_cb(err):\n    \"\"\" The error callback is used for generic client errors. These\n        errors are generally to be considered informational as the client will\n        automatically try to recover from all errors, and no extra action\n        is typically required by the application.\n        For this example however, we terminate the application if the client\n        is unable to connect to any broker (_ALL_BROKERS_DOWN) and on\n        authentication errors (_AUTHENTICATION). \"\"\"\n\n    print(\"Client error: {}\".format(err))\n    if err.code() == KafkaError._ALL_BROKERS_DOWN or \\\n       err.code() == KafkaError._AUTHENTICATION:\n        # Any exception raised from this callback will be re-raised from the\n        # triggering flush() or poll() call.\n        raise KafkaException(err)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c92e614d-6dd6-40bd-9a49-6b85ce981a00"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Helpful Links\n\n* [Confluent's github repo](https://github.com/confluentinc/confluent-kafka-python) - Confluent's github repo of code examples for python Kafka examples, includes almost everything needed for core development with Kafka\n* [Docstring Documentation](https://www.datacamp.com/community/tutorials/docstrings-python) - Comments on Page made in Docstring"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc2c7cb3-2f01-4bf7-9da1-a2e85a22064b"}}},{"cell_type":"markdown","source":["**Kafka Consumer Setup**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2dca4a6-90b1-48fe-92c0-0d9853fb4ffa"}}},{"cell_type":"code","source":["from confluent_kafka import Consumer\nfrom time import sleep\nimport uuid\nfrom confluent_kafka import Producer, Consumer, KafkaError, KafkaException\nimport json\n\n\n#KAFKA variables, Move to the OS variables or configuration\n# This will work in local Jupiter Notebook, but in a databrick, hiding config.py is tougher. \nconfluentClusterName = \"stage3talent\"\nconfluentBootstrapServers = \"pkc-ldvmy.centralus.azure.confluent.cloud:9092\"\n#confluentTopicName = \"cell-jed\"\nconfluentTopicName = \"arctic_analysts_main_table\"\nschemaRegistryUrl = \"https://psrc-gq7pv.westus2.azure.confluent.cloud\"\nconfluentApiKey = \"YHMHG7E54LJA55XZ\"\nconfluentSecret = \"/XYn+w3gHGMqpe9l0TWvA9FznMYNln2STI+dytyPqtZ9QktH0TbGXUqepEsJ/nR0\"\nconfluentRegistryApiKey = \"YHMHG7E54LJA55XZ\"\nconfluentRegistrySecret = \"/XYn+w3gHGMqpe9l0TWvA9FznMYNln2STI+dytyPqtZ9QktH0TbGXUqepEsJ/nR0\"\n\n\n#Kakfa Class Setup.\nc = Consumer({\n    'bootstrap.servers': confluentBootstrapServers,\n    'sasl.mechanism': 'PLAIN',\n    'security.protocol': 'SASL_SSL',\n    'sasl.username': confluentApiKey,\n    'sasl.password': confluentSecret,# this will create a new consumer group on each invocation.\n    'group.id': str(1),\n    'auto.offset.reset': 'earliest',\n    'enable.auto.commit': True,\n    'error_cb': error_cb,\n})\n\n#c.subscribe(['cell-jed'])\nc.subscribe(['arctic_analysts_main_table'])\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f404b3f4-2a72-4439-8367-e6564443e5fc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Define the received string storage mode and a list to contain received items**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3cbb031-0638-47cd-8c23-d3f9761659b8"}}},{"cell_type":"code","source":["# Function to check for messages until 1000, and then commiting message\n\ndef check_for_messages():\n    aString = {}\n    kafkaListDictionaries = []\n\n    i = 0\n    while True:\n        if len(kafkaListDictionaries) > 1000: # Bumped up for error checking\n            result = 'update'\n            break\n        try:\n            # Pause and wait for new messages\n            msg = c.poll(timeout=15)\n\n            # If timeout and no new messages\n            if msg is None:\n                i += 1\n                plural = \".\" if 10 - i == 1 else \"s.\"\n                print(f'{15*i} seconds elapsed without a response.')\n                if i == 10:\n                    result = 'timeout'\n                    break\n\n                print(f'Stopping Consumer after {10 - i} more attempt{plural}')\n\n            elif msg.error():\n                print(\"Consumer error: {}\".format(msg.error()))\n                break\n            else:\n                i = 0\n                print(f'Message {len(kafkaListDictionaries)} successfully received')\n                aString=json.loads('{}'.format(msg.value().decode('utf-8')))\n#                 aString['timestamp'] = msg.timestamp()[1]\n                print(aString)\n                kafkaListDictionaries.append(aString)\n\n                c.commit(asynchronous=False)\n        except Exception as e:\n            print(e)\n            break\n    print('Consumer Stopped')\n    return kafkaListDictionaries, result"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed23afc9-051b-4d8a-9ea0-a78c51ff97d5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function to read from database table to check to only write new data to database table\n\ndef add_to_database(dataframe):\n    database = \"arctic_analysts_capstone\"\n    table = \"dbo.main_table\"\n    user = \"arctic_analysts\"\n    password  = \"ThisPassw0rd!\"\n    server = \"gen10-data-fundamentals-22-02-sql-server.database.windows.net\"\n\n    \n    read_from_df = spark.read.format(\"jdbc\") \\\n      .option(\"url\", f\"jdbc:sqlserver://{server}:1433;databaseName={database};\") \\\n      .option(\"dbtable\", table) \\\n      .option(\"user\", user) \\\n      .option(\"password\", password) \\\n      .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n      .load()\n    \n    write_df = dataframe.join(read_from_df, ['FIPS','YearID','MonthID'],'left_anti')\n\n    # WRITE <--- dataframe to database\n    write_df.write.format(\"jdbc\") \\\n      .option(\"url\", f\"jdbc:sqlserver://{server}:1433;databaseName={database};\") \\\n      .mode(\"append\") \\\n      .option(\"dbtable\", table) \\\n      .option(\"user\", user) \\\n      .option(\"password\", password) \\\n      .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n      .save()\n    print('Successfully added the data to the dataframe.')\n\n    \n#     READ <--- FROM DATABASE\n#     jdbc = spark.read.format(\"jdbc\") \\\n#       .option(\"url\", f\"jdbc:sqlserver://{server}:1433;databaseName={database};\") \\\n#       .option(\"dbtable\", table) \\\n#       .option(\"user\", user) \\\n#       .option(\"password\", password) \\\n#       .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n#       .load()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e613389-0e04-422f-bae4-0d27b16df1ef"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Calling on functions to check for new messages and writing to database\n\nresult = 'started'\nwhile result != 'timeout':\n    \n    kafkaListDictionaries, result = check_for_messages()\n    print(result)\n    \n    if len(kafkaListDictionaries) > 0:\n        try:\n#             df = convert_to_dataframe(kafkaListDictionaries)\n            df = spark.createDataFrame(kafkaListDictionaries)\n            add_to_database(df)\n        except Exception as E:\n            print(E)\n            print('Process Failure')\n            break\n    else:\n        print('There is no new data.')\n        \nprint('Finished Consuming')\n# FYI, I have built a pause into the producer at i % 100 == 0, so that is why it occasionally doesn't find any messages."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8d7f354-d651-4cc2-a05a-a829eebc7489"}},"outputs":[],"execution_count":0}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.8","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3.6.8 64-bit ('PythonData': conda)","name":"python3"},"orig_nbformat":4,"application/vnd.databricks.v1+notebook":{"notebookName":"arctic_capstone_consumer","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3135277668778985},"interpreter":{"hash":"f0aeb945a0f4941527bbb9f5e68988ff233b4c4d67ade5fbe62231d824151796"}},"nbformat":4,"nbformat_minor":0}

{"cells":[{"cell_type":"markdown","source":["## Data Joins\n**Intended Utility**\n>This databrick is intended to prepare all tables for input into the database.<br>\n\n>This will involve joining some of the tables together, and sending some directly to the database<br>\n>while others are sent through Kafka."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c6783fc-df26-4dbe-b8bf-1963952be1bf"}}},{"cell_type":"markdown","source":["**Configuration:**\n> These cells are responsible for configuring the primary aspects of the databrick.<br>\n\n**Config Part 1:** Import Libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e157b1ce-effb-4963-a3ae-0bde7a383f39"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43d915bf-0d6a-4650-bf10-fef4d60d37ab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Config Part 2:** Create I/O widgets for pipeline."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c053cb2-a5bc-4991-b818-c44be3c7e58e"}}},{"cell_type":"code","source":["# These widgets allow the pipeline to set the file I/O information.\n\ndbutils.widgets.text(\"input\", \"\",\"\") \ndbutils.widgets.get(\"input\")\n \ndbutils.widgets.text(\"output\", \"\",\"\") \ndbutils.widgets.get(\"output\")\n \ndbutils.widgets.text(\"filename\", \"\",\"\") \ndbutils.widgets.get(\"filename\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf135fdb-4e65-43e6-9c0d-2d213fdfe898"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[19]: &#39;&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[19]: &#39;&#39;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Config Part 3:** Designate Mount Points"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47b7c4d6-03c3-4102-88f1-33a66cb832cd"}}},{"cell_type":"code","source":["def mount_storage(mount_goal):\n    storageAccount = mount_goal['account']\n    storageContainer = mount_goal['container']\n    clientSecret = \"B4g8Q~1VyZJa5WszLHwdEQNq4YIaHmT4DevRBcwI\"\n    clientid = \"2ca50102-5717-4373-b796-39d06568588d\"\n    mount_point = mount_goal['mount']\n\n    configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n           \"fs.azure.account.oauth2.client.id\": clientid,\n           \"fs.azure.account.oauth2.client.secret\": clientSecret,\n           \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/d46b54b2-a652-420b-aa5a-2ef7f8fc706e/oauth2/token\",\n           \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\n\n    try: \n        dbutils.fs.unmount(mount_point)\n    except:\n        pass\n\n    dbutils.fs.mount(\n    source = \"abfss://\"+storageContainer+\"@\"+storageAccount+\".dfs.core.windows.net/\",\n    mount_point = mount_point,\n    extra_configs = configs)\n    \n    return mount_point\n    \nin_path = getArgument(\"input\").split(\"/\")\nout_path = getArgument(\"output\").split(\"/\")\n\nstorage_info = {\n    'read': {'account': in_path[0], 'container': in_path[1], 'mount': \"/mnt/arctic_analysts_final_prep_read\"},\n    'write': {'account': out_path[0], 'container': out_path[1], 'mount': \"/mnt/arctic_analysts_final_prep_write\"}\n}\n\ntry:\n    read_path = mount_storage(storage_info['read'])\n    write_path = mount_storage(storage_info['write'])\nexcept Exception as E:\n    print(E[:-50])\n    \nprint(f'Read Path: {read_path}\\nWrite Path: {write_path}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3786734-d5ad-481d-8dc9-49b43e03f3a6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/mnt/arctic_analysts_final_prep_read has been unmounted.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/arctic_analysts_final_prep_read has been unmounted.\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Read all the cleaned files from the blob.\nfilenames = {\n    'building_permits': read_path + \"/\" + 'county_level_building_permits_from_databrick.json',\n    'house_prices': read_path + \"/\" + 'county_level_house_prices_from_databrick.json',\n    'median_income': read_path + \"/\" + 'median_income_data_from_databrick.json',\n    'mortgage_data': read_path + \"/\" + 'mortgage_data_from_databrick.json'\n}\n\nbuilding_permits = spark.read.json(filenames['building_permits'])\nhouse_prices = spark.read.json(filenames['house_prices'])\nmedian_income = spark.read.json(filenames['median_income'])\nmortgage_data = spark.read.json(filenames['mortgage_data'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a23dd18-7449-43e9-90b4-b9c78739b540"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Join Tables and clean for storage**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d44b80c-7123-45d3-9192-f51b82b21f74"}}},{"cell_type":"code","source":["def prepare_tables(mortgage_data, building_permits, house_prices):\n    month_dict = {\n        'JANUARY': 'Jan',\n        'FEBRUARY': 'Feb',\n        'MARCH': 'Mar',\n        'APRIL': 'Apr',\n        'MAY': 'May',\n        'JUNE': 'Jun',\n        'JULY': 'Jul',\n        'AUGUST': 'Aug',\n        'SEPTEMBER': 'Sep',\n        'OCTOBER': 'Oct',\n        'NOVEMBER': 'Nov',\n        'DECEMBER': 'Dec',    \n    }\n\n    abbreviate = udf(lambda x: month_dict[x])\n    mortgage_data = mortgage_data.withColumn(\"Month\", abbreviate(col(\"Month\")))\n\n    # Rename columns to indicate those that will be dropped after table join\n    house_price_columns = ['County_DROP','FIPS_Drop','MedianHousePrice','Month_drop','Year_drop']\n    house_prices = house_prices.toDF(*house_price_columns)\n\n    # Rename columns to indicate those that will be dropped after table join\n    mortgage_data_columns = ['AveragePoints','AverageRate','Month_Drop','Year_Drop']\n    mortgage_data = mortgage_data.toDF(*mortgage_data_columns)\n\n    # Join Tables\n    main_table = building_permits.join(house_prices, (building_permits.FIPS == house_prices.FIPS_Drop) & (building_permits.Year == house_prices.Year_drop) & (building_permits.Month == house_prices.Month_drop))\n    main_table = main_table.join(mortgage_data, (main_table.Month == mortgage_data.Month_Drop) & (main_table.Year == mortgage_data.Year_Drop))\n\n    # Drop columns that are not needed\n    cols_to_drop = [_ for _ in main_table.columns if ('_Drop' in _ or '_drop' in _ or '_DROP' in _ or 'Date' in _)]\n    main_table = main_table.drop(*cols_to_drop)\n\n\n    # Break year and month out into separate tables\n    year = main_table.select(\"Year\").distinct().sort(\"Year\")\n    month = main_table.select(\"Month\").distinct()\n\n    # indexing the months for sorting\n    month_dict = {\n        'Jan': 1,\n        'Feb': 2,\n        'Mar': 3,\n        'Apr': 4,\n        'May': 5,\n        'Jun': 6,\n        'Jul': 7,\n        'Aug': 8,\n        'Sep': 9,\n        'Oct': 10,\n        'Nov': 11,\n        'Dec': 12 \n    }\n    assign_month = udf(lambda x: int(month_dict[x]))\n    month = month.withColumn(\"MonthID\", assign_month(col(\"Month\")))\n\n    county = main_table.select(\"FIPS\",\"County\").distinct()\n\n    main_table = main_table.withColumn(\"MonthID\", assign_month(col(\"Month\")))\n    main_table = main_table.drop(\"County\",\"Month\")\n    \n    main_table = main_table.withColumn(\"YearID\", col(\"Year\")-1999)\n    main_table = main_table.drop(\"Year\")\n    return main_table, year, month, county\n    \ndef prepare_income_table():\n    median_income = median_income.drop(\"County\")\n    median_income = median_income.withColumn(\"YearID\", col(\"Year\")-1999)\n    median_income = median_income.drop(\"Year\")\n    return median_income"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe797a0c-0451-4920-b556-3bed8ba439a3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["def add_to_database(df, write_method, target_table):\n    database = \"arctic_analysts_capstone\"\n    table = f\"dbo.{target_table}\"\n    user = \"arctic_analysts\"\n    password  = \"ThisPassw0rd!\"\n    server = \"gen10-data-fundamentals-22-02-sql-server.database.windows.net\"\n\n    # WRITE <--- dataframe to database\n    df.write.format(\"jdbc\") \\\n      .option(\"url\", f\"jdbc:sqlserver://{server}:1433;databaseName={database};\") \\\n      .mode(write_method) \\\n      .option(\"dbtable\", table) \\\n      .option(\"user\", user) \\\n      .option(\"password\", password) \\\n      .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n      .save()\n    print('Successfully added the data to the dataframe.')\n\ndef read_from_database(target_table = 'main_table'):\n    database = \"arctic_analysts_capstone\"\n    table = f\"dbo.{target_table}\"\n    user = \"arctic_analysts\"\n    password  = \"ThisPassw0rd!\"\n    server = \"gen10-data-fundamentals-22-02-sql-server.database.windows.net\"\n\n    jdbc = spark.read.format(\"jdbc\") \\\n        .option(\"url\", f\"jdbc:sqlserver://{server}:1433;databaseName={database};\") \\\n        .option(\"dbtable\", target_table) \\\n        .option(\"user\", user) \\\n        .option(\"password\", password) \\\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n        .load()\n    return jdbc\n  \ndef save_new_data(df):   \n    # Define write path\n    final_path = write_path + '/main_table_prepped_for_kafka'\n\n    # Reduce to single file and write to blob\n    df.repartition(1).write.format(\"com.databricks.spark.json\") \\\n    .mode('overwrite').option(\"header\", \"True\") \\\n    .json(final_path)\n\n    # Locate file in blob\n    files = dbutils.fs.ls(final_path)\n    json_file = [x.path for x in files if x.path.endswith(\".json\")][0]\n\n    # Move file out of directory into main blob and delete junk files\n    dbutils.fs.mv(json_file, final_path + \".json\")\n    dbutils.fs.rm(final_path, recurse = True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e53847f4-f2bc-4731-b195-7bdf6130a2c7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Validate Update\n> This code block checks the data in the blob to see if it can be sent to the database."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc0c8db8-1a0d-4432-916d-9f5834b083fd"}}},{"cell_type":"code","source":["def validate_update(new_data):\n\n    existing_data = read_from_database()\n    \n    # Get the dates of the old data\n    greatest_year_id = max(set(existing_data.select('YearID').rdd.flatMap(lambda x: x).collect()))\n    existing_data = existing_data.where(col(\"YearID\") == greatest_year_id)\n    greatest_month_id = max(set(existing_data.select('MonthID').rdd.flatMap(lambda x: x).collect()))\n    current_data = [greatest_year_id, greatest_month_id]\n\n    # Get the dates of the new data\n    greatest_year_id = max(set(new_data.select('YearID').rdd.flatMap(lambda x: x).collect()))\n    filtered_new_data = new_data.where(col(\"YearID\") == greatest_year_id)\n    greatest_month_id = max(set(filtered_new_data.select('MonthID').rdd.flatMap(lambda x: x).collect()))\n    new_data_dates = [int(greatest_year_id), int(greatest_month_id)]\n\n    # Check if an update is warranted.\n    if current_data == new_data_dates:\n        # If the year and month in the new data is the same as old\n        print('There is no new data.')\n        rule = 'no_update'\n    elif new_data_dates[0] > current_data_dates[0]:\n        # If the year in the data is greater than the year in the old data\n        print('There is new data')\n        rule = 'append'\n    elif (new_data_dates[0] >= current_data_dates[0] and new_data_dates[1] > current_data_dates[1]):\n        # If the year in the new data is greater than or equal to the year in the old data and \n        # if the month in the new data is greater than the month in the old data\n        print('There is new data')\n        rule = 'append'\n    return rule, current_data\n\n# Get the data and prepare it for storage.\nnew_data, year, month, county = prepare_tables(mortgage_data, building_permits, house_prices)\n\ntry:\n    rule, current_data = validate_update(new_data)\nexcept:\n    print('There must be no data, we will overwrite the table.')\n    rule = 'overwrite'\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a17f9105-ba33-4a00-be88-a2ebbf2849f9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["def filter_data(main_table, median_income, current_data):\n    main_table = main_table.where((col(\"YearID\") >= current_data[0]) & (col('MonthID') >= current_data[1]))\n    median_income = median_income.where((col(\"YearID\") >= current_data[0]) & (col('MonthID') >= current_data[1]))\n    return main_table, median_income\n\nif rule == 'append':\n    #filter down the table to just the new data\n    print('Filtering for appending.')\n    main_table, median_income = filter_data(new_data, median_income, current_data)\n    \n    # Update the year and median income\n    add_to_database(year, 'overwrite', 'year') \n    add_to_database(median_income, 'append', 'median_income')\n    \n    # Save the table in the blob for kafka to pick up\n    save_new_data(main_table)\n    \nelif rule == 'overwrite':\n    add_to_database(year, rule, 'year')    \n    add_to_database(month, rule, 'month')\n    add_to_database(county, rule, 'county')\n    add_to_database(median_income, rule, 'median_income')\n    \n    # Save the table in the blob for kafka to pick up\n    save_new_data(main_table)\n    \n    #send the whole table\n    print('Overwriting the table')\nelif rule == 'no_update':\n    print('No update')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f7b6498-9029-4af3-bb41-97ea3ed4547a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# This will be sent through kafka\n#add_to_database(main_table, 'main_table')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a7261c9-1536-4b2f-836f-9ac89ca52d73"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(median_income)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ae4f948-34e2-4374-9c35-1901d2e20a18"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8bf2974-6e6e-486f-aae2-a8bb4c0d7ba5"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"arctic_capstone_data_joins","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"widgetLayout":[]},"language":"python","widgets":{"input":{"nuid":"8d0f630d-cb46-492c-9135-9cbf8d703411","currentValue":"gen10datafund2202/z-arctic-capstone-cleaned-data","widgetInfo":{"widgetType":"text","name":"input","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"filename":{"nuid":"24517d80-b4bb-4c6e-8793-c3ca69cd3fba","currentValue":"","widgetInfo":{"widgetType":"text","name":"filename","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"output":{"nuid":"db60cc42-8825-45f8-8809-bd1f11efe72b","currentValue":"gen10datafund2202/z-arctic-analysts-capstone-backup","widgetInfo":{"widgetType":"text","name":"output","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":4117336921112779}},"nbformat":4,"nbformat_minor":0}

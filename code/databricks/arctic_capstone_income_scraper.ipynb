{"cells":[{"cell_type":"markdown","source":["## US Census Median Income API Access \n**Intended Utility**\n>This databrick is designed to be utilized as part of a pipeline.<br>\n\n>US Census Data is not updated frequently, so this tool will only <br>\n>allow for access attempts every 6 months. Changing this behavior will<br>\n>require syntax override or removing file history to throw an exception."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42287886-7c8a-401e-85e3-80e386ca2c34"}}},{"cell_type":"markdown","source":["**Configuration:**\n> These cells are responsible for configuring the primary aspects of the databrick.<br>\n\n**Config Part 1:** Import Libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"199d3c00-de7a-4cfb-8cc2-b45ee8811ce6"}}},{"cell_type":"code","source":["import requests\nimport pyspark.pandas as ps\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.functions import array, col, explode, lit, struct, concat_ws, StringType, split\nfrom pyspark.sql import DataFrame\nfrom typing import Iterable \nimport datetime as dt\nimport time"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"224ed217-69bf-4d22-a026-8ff28234420b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Config Part 2:** Create I/O widgets for pipeline."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"359c3f7f-ee8c-47ed-bd28-d14e6949e2e4"}}},{"cell_type":"code","source":["# These widgets allow the pipeline to set the file I/O information.\n\ndbutils.widgets.text(\"input\", \"\",\"\") \ndbutils.widgets.get(\"input\")\n \ndbutils.widgets.text(\"output\", \"\",\"\") \ndbutils.widgets.get(\"output\")\n \ndbutils.widgets.text(\"filename\", \"\",\"\") \ndbutils.widgets.get(\"filename\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ce48e02-9661-427a-926d-42209fe354a7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[52]: &#39;median_income_data_from_databrick&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[52]: &#39;median_income_data_from_databrick&#39;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Config Part 3:** Designate Mount Points"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac5fad0e-8d26-4ae7-bfc7-021f942d4968"}}},{"cell_type":"code","source":["def mount_storage(mount_goal):\n    storageAccount = mount_goal['account']\n    storageContainer = mount_goal['container']\n    clientSecret = \"B4g8Q~1VyZJa5WszLHwdEQNq4YIaHmT4DevRBcwI\"\n    clientid = \"2ca50102-5717-4373-b796-39d06568588d\"\n    mount_point = mount_goal['mount']\n\n    configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n           \"fs.azure.account.oauth2.client.id\": clientid,\n           \"fs.azure.account.oauth2.client.secret\": clientSecret,\n           \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/d46b54b2-a652-420b-aa5a-2ef7f8fc706e/oauth2/token\",\n           \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\n\n    try: \n        dbutils.fs.unmount(mount_point)\n    except:\n        pass\n\n    dbutils.fs.mount(\n    source = \"abfss://\"+storageContainer+\"@\"+storageAccount+\".dfs.core.windows.net/\",\n    mount_point = mount_point,\n    extra_configs = configs)\n    \n    return mount_point\n    \nin_path = getArgument(\"input\").split(\"/\")\nout_path = getArgument(\"output\").split(\"/\")\n\nstorage_info = {\n    'read': {'account': in_path[0], 'container': in_path[1], 'mount': \"/mnt/arctic_analysts_income_scraper_read\"},\n    'write': {'account': out_path[0], 'container': out_path[1], 'mount': \"/mnt/arctic_analysts_income_scraper_write\"},\n    #'check': {'account': out_path[0], 'container': out_path[1], 'mount': \"/mnt/arctic_analysts_income_scraper_check\"}\n}\n\ntry:\n    read_path = mount_storage(storage_info['read'])\n    write_path = mount_storage(storage_info['write'])\n    #check_path = mount_storage(storage_info['check'])\nexcept Exception as E:\n    print(E[:-50])\n    \nprint(f'Read Path: {read_path}\\nWrite Path: {write_path}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11cadf95-0b1c-4426-abc4-839d2c9ffbae"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/mnt/arctic_analysts_income_scraper_read has been unmounted.\n/mnt/arctic_analysts_income_scraper_write has been unmounted.\nRead Path: /mnt/arctic_analysts_income_scraper_read\nWrite Path: /mnt/arctic_analysts_income_scraper_write\nCheck Path: /mnt/arctic_analysts_income_scraper_check\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/arctic_analysts_income_scraper_read has been unmounted.\n/mnt/arctic_analysts_income_scraper_write has been unmounted.\nRead Path: /mnt/arctic_analysts_income_scraper_read\nWrite Path: /mnt/arctic_analysts_income_scraper_write\nCheck Path: /mnt/arctic_analysts_income_scraper_check\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Data Access Validation**\n>These cells check if an api call is reasonable.<br>\n\n**Data Check Part 1:** Get the list of available files from the source."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce0d5081-0466-48fa-9748-8763edd8120c"}}},{"cell_type":"code","source":["#Validation\n# need to get the list of years from the database, and then scrape only future years\n\ndef read_from_database(table):\n    database = \"arctic_analysts_capstone\"\n    table = f\"dbo.{table}\"\n    user = \"arctic_analysts\"\n    password  = \"ThisPassw0rd!\"\n    server = \"gen10-data-fundamentals-22-02-sql-server.database.windows.net\"\n\n    jdbc = spark.read.format(\"jdbc\") \\\n        .option(\"url\", f\"jdbc:sqlserver://{server}:1433;databaseName={database};\") \\\n        .option(\"dbtable\", table) \\\n        .option(\"user\", user) \\\n        .option(\"password\", password) \\\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n        .load()\n    return jdbc\n\n# Get current year\ncurrent_year = dt.datetime.today().year\n\n# Check for any available years that are less than the current year, but not in the database.\ntry:\n    years = read_from_database('year')\n    median_income = read_from_database('median_income')\n\n    income_years = years.join(median_income, (years.YearID == median_income.YearID))\n    most_recent_year = max(income_years.select('Year').rdd.flatMap(lambda x: x).collect())\n    allowed_attempts = [_ for _ in range(most_recent_year + 1, current_year)]\n    rule = 'scrape_new'\nexcept:\n    # If exceptions, assume that the data is missing and we need to repopulate it.\n    rule = 'scrape_all'\n    allowed_attempts = [_ for _ in range(2005,current_year)]\nprint(allowed_attempts)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14967647-9cb4-44b9-aa01-5dda5e4e9a58"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[2020, 2021]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[2020, 2021]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## API Access\n>We will access the US Census API by first getting the list of variables<br>\n>for the table that we are interested in for each year. The reason for this step<br>\n>is that the variables can change over the years, and we want to make sure that we have<br>\n>the correct variable for each year.<br>\n\n>We are then creating a list of endpoints of the variables that we retrieve from the variable<br>\n>table, and passing that list into the api call."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3707162-be09-4043-89bc-761261c31fa6"}}},{"cell_type":"code","source":["groups = ['B19049']\ndef get_var_dict(acs_variables):\n    \"\"\" This function will return the list of variables and what they are to the primary api routine. \"\"\"\n    name_list = acs_variables[(acs_variables.Group.isin(groups))][['Name','Label', 'Concept']].Name.tolist()\n    label_list = acs_variables[(acs_variables.Group.isin(groups))][['Name','Label', 'Concept']].Label.tolist()\n\n    var_dict = {}\n    for i, name in enumerate(name_list):\n        label = label_list[i]\n        label = label.replace('Estimate!!','').replace(\":\",\"\").replace(\"Total!!\",\"\").replace(\"$\",\"\") \n\n        add_on = '_inflation_adjusted_in_data_year'\n        \n        label = label.split('!!')[1]\n        var_dict[name] = label + add_on\n     \n    return var_dict"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f848d67-68dc-4342-8d4c-660608bc24f2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_median_income_data(year):\n    # We need to get the variables from each unique year to ensure that any changes are caught.\n    acs_variables = ps.read_html(f'https://api.census.gov/data/{year}/acs/acs1/variables.html')[0]\n   \n    # Create a string of endpoints that will be passed to the api call.\n    selected_endpoints = acs_variables[(acs_variables.Group.isin(groups))].Name.tolist()\n    endpoints = \",\".join(selected_endpoints)\n    \n    # Get the variables and their information.\n    var_dict = get_var_dict(acs_variables)\n    \n    # Call the census api (might need to add this to a while loop to check a few time in case of exceptions)\n    geography = 'county'\n    acs_api = f'https://api.census.gov/data/{year}/acs/acs1?get=NAME,{endpoints}&for={geography}:*'\n    data = requests.get(acs_api).json()\n    \n    # Create a dataframe from the api_response.\n    api_response_df = spark.createDataFrame(data)\n\n    # Get top row\n    vals = api_response_df.limit(1).collect()[0].asDict()\n\n    # Create New DF with renamed columns\n    new_column_names = [vals[key] for key in vals]\n    cleaned_column_names = []\n    for column_name in new_column_names:\n        if column_name in var_dict:\n            cleaned_column_names.append(var_dict[column_name])\n        else:\n            cleaned_column_names.append(column_name)\n\n    new_df = api_response_df.toDF(*cleaned_column_names)\n    \n    # Filter out unneeded rows and assign the year to a column.\n    new_df = new_df.where(new_df.NAME != 'NAME')\n    new_df = new_df.withColumn(\"YEAR\", lit(year))\n        \n    return new_df\n\n\ndef melt(\n    df: DataFrame,\n    id_vars: Iterable[str], value_vars: Iterable[str],\n    var_name: str=\"AgeGroup\",\n    value_name: str=\"MedianIncome\"):\n    \"\"\" This function converts the data from wide to long form. \"\"\"\n    \n    _vars_and_vals = array(*(\n        struct(lit(c).alias(var_name), col(c).alias(value_name))\n        for c in value_vars))\n    \n    _tmp = df.withColumn(\"_vars_and_vals\", explode(_vars_and_vals))\n    \n    cols = id_vars + [\n        col(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n    \n    return _tmp.select(*cols)\n\n  \ndef clean_data(df):\n    \n    new_column_names = {\n        'Householder 25 to 44 years_inflation_adjusted_in_data_year': '25-44',\n        'Householder 45 to 64 years_inflation_adjusted_in_data_year':  '45-64',\n        'Householder 65 years and over_inflation_adjusted_in_data_year': '65-plus',\n        'Householder under 25 years_inflation_adjusted_in_data_year': 'under-25',\n        'NAME': 'County',\n        'Total_inflation_adjusted_in_data_year': 'overall',\n        'YEAR': 'Year',\n        'county': 'CountyFips',\n        'state': 'StateFips'\n    }\n    \n    # Assign new column names\n    new_columns = [new_column_names[_] for _ in df.columns]\n    new_df = df.toDF(*new_columns)\n    \n    # Convert the data from wide form to long form.\n    melted_frame = melt(new_df, id_vars = ['Year','County','CountyFips','StateFips'], value_vars = ['25-44','45-64','65-plus','under-25','overall'])\n    \n    # Combine state and county fips to be one fips.\n    other_columns = [_ for _ in melted_frame.columns if _ not in ['StateFips','CountyFips']]\n    final_frame = melted_frame.select(concat_ws('', melted_frame.StateFips, melted_frame.CountyFips).alias('FIPS'),*other_columns)\n    \n    # Filter for New Jersey Data\n    final_frame = final_frame.where(col(\"FIPS\").like('34%'))\n    \n    # Split the \"County\" column to get only the only county name\n    final_frame = final_frame.withColumn(\"County\", split(col(\"County\"),\",\").getItem(0))\n    return final_frame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a17849b4-62d6-4e05-8878-934e6ea89e14"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def save_data_to_blob(df, method):\n    final_path = write_path + '/median_income_data_from_databrick'\n\n    # Reduce to single file and write to blob\n    df.repartition(1).write.format(\"com.databricks.spark.json\") \\\n    .mode(method).option(\"header\", \"True\") \\\n    .json(final_path)\n\n    # Locate file in blob\n    files = dbutils.fs.ls(final_path)\n    json_file = [x.path for x in files if x.path.endswith(\".json\")][0]\n\n    # Move file out of directory into main blob and delete junk files\n    dbutils.fs.mv(json_file, final_path + \".json\")\n    dbutils.fs.rm(final_path, recurse = True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efeb158d-7cc3-4eaf-8718-8f4c75786a3e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## API Call Controller\n> This code block controls the api calls"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"164c5bb8-8728-468c-9771-95c95e4de094"}}},{"cell_type":"code","source":["errors = 0\nprimary_df = None\nfor i, year in enumerate(allowed_attempts):\n    print(f'Gathering {i+1} of {len(allowed_attempts)} | {year}')\n    try:\n        if i == 0:\n            primary_df = get_median_income_data(year)\n        else:\n            primary_df = primary_df.unionByName(get_median_income_data(year))\n            \n        print(primary_df.count())\n    except Exception as E:\n        errors += 1\n        print(E)\n        if errors > 5:\n            print(\"Exiting due to high number of exceptions.\")\n            break\n        \n\n# Clean the data    \n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2596b6a0-64ff-49ed-bb59-2d7136b2bf77"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Gathering 1 of 2 | 2020\nHTTP Error 404: Not Found\nGathering 2 of 2 | 2021\n&#39;NoneType&#39; object has no attribute &#39;unionByName&#39;\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Gathering 1 of 2 | 2020\nHTTP Error 404: Not Found\nGathering 2 of 2 | 2021\n&#39;NoneType&#39; object has no attribute &#39;unionByName&#39;\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##Full Clean\n> This code block sends the data to a cleaning function if necessary."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fac0a2d8-5814-4f0c-835e-4baeb6558da6"}}},{"cell_type":"code","source":["if rule == 'scrape_all':\n    if primary_df.count() > 0:\n        finished_df = clean_data(primary_df)   \n    #     call_history_df = create_history()\n    #     update_history(call_history_df, 'overwrite')\n        save_data_to_blob(finished_df, 'overwrite')\n    else:\n        print('No new data.')\nelse:\n    try:\n        if primary_df.count() > 0:\n            finished_df = clean_data(primary_df)\n            save_data_to_blob(finished_df, 'append')\n            print('Blob Updates')\n    except Exception as E:\n        print(E)\n        print(\"Nothing to update.\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfac67f1-2638-4dbf-83f1-4d52241253e5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&#39;NoneType&#39; object has no attribute &#39;count&#39;\nNothing to update.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&#39;NoneType&#39; object has no attribute &#39;count&#39;\nNothing to update.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# If we want to read the data from the blob\n#income_df = spark.read.options(inferSchema='True', header='True').json(read_path + \"/\" + getArgument('filename') + \".json\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4ee354c-8402-46c4-ad0e-bb637d460c63"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a91bb5d-5989-4f2b-bec3-2bef7256dae8"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"arctic_capstone_income_scraper","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{"input":{"nuid":"b9a7e94f-0fcb-4611-9dc3-1d1d71a466b8","currentValue":"gen10datafund2202/z-arctic-analysts-capstone-main","widgetInfo":{"widgetType":"text","name":"input","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"filename":{"nuid":"1fc1d463-f3b6-4a19-af93-ef32e39e1c52","currentValue":"median_income_data_from_databrick","widgetInfo":{"widgetType":"text","name":"filename","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"output":{"nuid":"014c41d1-f39d-4fad-98e4-767881274057","currentValue":"gen10datafund2202/z-arctic-capstone-cleaned-data","widgetInfo":{"widgetType":"text","name":"output","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":1967226895815899}},"nbformat":4,"nbformat_minor":0}

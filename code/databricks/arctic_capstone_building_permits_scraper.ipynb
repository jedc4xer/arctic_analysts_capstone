{"cells":[{"cell_type":"markdown","source":["## US Census Data Building Permit Scraper\n**Intended Utility**\n>This databrick is designed to be utilized as part of a pipeline.<br><br>\n>The data can be scraped at any point to check for updates. The databrick will check<br>\n>the available files, and check a scraped list in the container, and if there is a new file<br>\n>then that file will be processed and the database will be updated.<br><br>\n>If there are no new files then the databrick will take no action."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da21210d-f3de-446c-bc92-2512375fd60f"}}},{"cell_type":"markdown","source":["**Configuration:**\n> These cells are responsible for configuring the primary aspects of the databrick.<br>\n\n**Config Part 1:** Import Libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6079e16e-a1ff-4dc0-812f-586935578893"}}},{"cell_type":"code","source":["import requests\nimport datetime as dt\nfrom operator import add\nfrom functools import reduce\nfrom bs4 import BeautifulSoup\nfrom pyspark import SparkFiles\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.functions import to_date\nfrom pyspark.sql.functions import concat_ws, trim"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d08ad938-1689-448f-8054-7cbca306005e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Config Part 2:** Create I/O widgets for pipeline."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9f43534-dc76-4cd2-9bbd-3b312033944b"}}},{"cell_type":"code","source":["# These widgets allow the pipeline to set the file I/O information.\n\ndbutils.widgets.text(\"input\", \"\",\"\") \ndbutils.widgets.get(\"input\")\n \ndbutils.widgets.text(\"output\", \"\",\"\") \ndbutils.widgets.get(\"output\")\n \ndbutils.widgets.text(\"filename\", \"\",\"\") \ndbutils.widgets.get(\"filename\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7b63821-d491-421b-b6e1-1287b1781bec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[61]: &#39;county_level_building_permits_from_databrick&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[61]: &#39;county_level_building_permits_from_databrick&#39;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Config Part 3:** Designate Mount Points"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e458a89c-9de2-4c6e-80bf-0d5f0a5862ae"}}},{"cell_type":"code","source":["def mount_storage(mount_goal):\n    storageAccount = mount_goal['account']\n    storageContainer = mount_goal['container']\n    clientSecret = \"B4g8Q~1VyZJa5WszLHwdEQNq4YIaHmT4DevRBcwI\"\n    clientid = \"2ca50102-5717-4373-b796-39d06568588d\"\n    mount_point = mount_goal['mount']\n\n    configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n           \"fs.azure.account.oauth2.client.id\": clientid,\n           \"fs.azure.account.oauth2.client.secret\": clientSecret,\n           \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/d46b54b2-a652-420b-aa5a-2ef7f8fc706e/oauth2/token\",\n           \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\n\n    try: \n        dbutils.fs.unmount(mount_point)\n    except:\n        pass\n\n    dbutils.fs.mount(\n    source = \"abfss://\"+storageContainer+\"@\"+storageAccount+\".dfs.core.windows.net/\",\n    mount_point = mount_point,\n    extra_configs = configs)\n    \n    return mount_point\n    \nin_path = getArgument(\"input\").split(\"/\")\nout_path = getArgument(\"output\").split(\"/\")\n\nstorage_info = {\n    'read': {'account': in_path[0], 'container': in_path[1], 'mount': \"/mnt/arctic_analysts_bps_scraper_read\"},\n    'write': {'account': out_path[0], 'container': out_path[1], 'mount': \"/mnt/arctic_analysts_bps_scraper_write\"},\n    'check': {'account': out_path[0], 'container': out_path[1], 'mount': \"/mnt/arctic_analysts_bps_scraper_check\"}\n}\n\ntry:\n    read_path = mount_storage(storage_info['read'])\n    write_path = mount_storage(storage_info['write'])\n    check_path = mount_storage(storage_info['check'])\nexcept Exception as E:\n    print(E[:-50])\n    \nprint(f'Read Path: {read_path}\\nWrite Path: {write_path}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76337724-c929-4d1d-b048-673fda63c787"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/mnt/arctic_analysts_bps_scraper_read has been unmounted.\n/mnt/arctic_analysts_bps_scraper_write has been unmounted.\n/mnt/arctic_analysts_bps_scraper_check has been unmounted.\nRead Path: /mnt/arctic_analysts_bps_scraper_read\nWrite Path: /mnt/arctic_analysts_bps_scraper_write\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/arctic_analysts_bps_scraper_read has been unmounted.\n/mnt/arctic_analysts_bps_scraper_write has been unmounted.\n/mnt/arctic_analysts_bps_scraper_check has been unmounted.\nRead Path: /mnt/arctic_analysts_bps_scraper_read\nWrite Path: /mnt/arctic_analysts_bps_scraper_write\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Database Data Check**\n>These cells check if there are any new files available.<br>\n\n**Data Check Part 1:** Get the list of available files from the source."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca49ca4d-879c-4796-8b32-a50f69eb6a96"}}},{"cell_type":"code","source":["primary_link = 'https://www2.census.gov/econ/bps/County/'\nsource = 'https://www2.census.gov'\n\nraw_html = requests.get(primary_link).text\nsoup = BeautifulSoup(raw_html)\n\nlinks = soup.find_all('a')\ndata_links = []\nfor link in links:\n    try:\n        # 'c' indicates data tabulated by month\n        # the other option is 'y' which is \n        # year-to-date summed data from the beginning\n        # of the year\n        if \"c.txt\" in link.get('href'):\n            data_links.append(primary_link + link.get('href'))\n    except:\n        continue"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0143113-abe0-4a75-a199-af40761cdc93"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Data Check Part 2:** Check if there are any new files available.\n> This code block will compare a previously created file and check if there are any new links<br>\n>from the previous step that are not in the previously gathered information."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce9a8c09-d610-4456-b098-702e221ab5a4"}}},{"cell_type":"code","source":["try:\n    previously_gathered = spark.read.options(inferSchema='True', header='True').json(check_path + '/scrape-history/bps_in_database.json')\n    previously_gathered_list = previously_gathered.select('InDatabase').rdd.flatMap(lambda x: x).collect()\n    new_links = [_ for _ in data_links if _ not in previously_gathered_list]\n    if len(new_links) == 0:\n        new_data = 'none'\n        print('Data is up to date.')\n    else:\n        new_data = 'update'\n        print('There is new data available.')\n        \nexcept Exception as E:\n    new_data = 'all'\n    new_links = data_links\n    print(E)\n    print('\\nThere seems to be no previously gathered data, or you deleted the list of previously collected files.')\n    print('An alternative to this check method would be to parse the date information from the links and see if the date is in the database.')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d87cce9-e410-4eb4-b81d-467ecc034b57"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Data is up to date.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Data is up to date.\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**New Data Gathering** <br>\n**Functions**\n>These are functions that manage how to scrape the data and what to do with it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6eb89a31-7804-43e6-9b25-51b4db0c7d82"}}},{"cell_type":"code","source":["def parse_BPS(link):\n    \"\"\" This function is responsible for converting a BPS text file to a Spark frame. \"\"\"\n    \n    # Get the filename from the link.\n    file = link.split(\"/\")[-1]\n    \n    # Add the file to the driver.\n    spark.sparkContext.addFile(link)\n    \n    # Read in the file from the driver.\n    df = spark.read.csv(\"file://\"+SparkFiles.get(file), header = True, inferSchema=True)\n    \n    # Get the values from the top row to add to the column names.\n    vals = df.limit(1).collect()[0].asDict()\n    \n    # Create new column names by combining old with new.\n    new_column_names = [(key + vals[key]).lower() for key in vals]\n    new_df = df.toDF(*new_column_names)\n    \n    # Drop the row with the unused data.\n    new_df = new_df.where(new_df.surveydate != 'Date')\n    return new_df\n\ndef process_new_data():\n    \"\"\" This function processes data from each link in \n    the list of new links that was collected during the data\n    validation phase.\n    \"\"\"\n    \n    # Count any errors\n    errors = 0\n    \n    # Create empty list for links that were successfully checked.\n    checked_links = []\n    \n    # Create empty list to store errors\n    error_links = []\n    \n    # Iterate through all of the links\n    for i, link in enumerate(new_links):\n        print(f\"Processing {i+1} of {len(data_links)}\")\n        try:\n            if i == 0:\n                # Attempt to parse the data from the link for the first link\n                response = parse_BPS(link)\n            else:\n                # Attempting to parse data from the next links\n                # then combining with the current master data frame \n                # which is called \"response\"\n                response = response.unionByName(parse_BPS(link))\n                \n            # If successful, then add the link to the checked links\n            checked_links.append(link)\n\n        except Exception as E:\n            error_links.append(link)\n            print(E)\n            errors += 1\n            if errors > 3:\n                print('More than 3 failures have occurred. Exiting Loop')\n                break\n        print(response.count())\n        \n    if len(error_links) > 0:\n        print('These are the links that caused errors.')\n        print(error_links)\n    return response, checked_links\n\ndef update_links(df, write_mode):\n    final_path = check_path + '/scrape-history/bps_in_database'\n\n    # Reduce to single file and write to blob\n    df.repartition(1).write.format(\"com.databricks.spark.json\") \\\n    .mode(write_mode).option(\"header\", \"True\") \\\n    .json(final_path)\n\n    # Locate file in blob\n    files = dbutils.fs.ls(final_path)\n    json_file = [x.path for x in files if x.path.endswith(\".json\")][0]\n\n    # Move file out of directory into main blob and delete junk files\n    dbutils.fs.mv(json_file, final_path + \".json\")\n    dbutils.fs.rm(final_path, recurse = True)\n    print(\"Updated the file containing scrape history.\")\n    \ndef save_new_data(df):   \n    # Define write path\n    final_path = write_path + '/county_level_building_permits_from_databrick'\n\n    # Reduce to single file and write to blob\n    df.repartition(1).write.format(\"com.databricks.spark.json\") \\\n    .mode(write_mode).option(\"header\", \"True\") \\\n    .json(final_path)\n\n    # Locate file in blob\n    files = dbutils.fs.ls(final_path)\n    json_file = [x.path for x in files if x.path.endswith(\".json\")][0]\n\n    # Move file out of directory into main blob and delete junk files\n    dbutils.fs.mv(json_file, final_path + \".json\")\n    dbutils.fs.rm(final_path, recurse = True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ca5a485-efa8-4483-9ae3-bd212f368576"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Dropping the Value and Reported from the dataset, keeping imputed data\n\ndef data_shower(df):\n    \n    # c2 references the column value from the original dataset, i.e. _c21, _c24\n    # Those columns held data related to the reported numbers which we are dropping\n    cols_to_drop = [_ for _ in df.columns if ('value' in _ or 'rep' in _ or '_c2' in _ or 'division' in _ or 'region' in _ or '_c18' in _)]\n    building_permit_df = df.drop(*cols_to_drop)\n\n    new_columns_names = {\n        '1-unitunits': '1_Unit',\n        '2-unitsunits': '2_Unit',\n        '3-4 unitsunits': '3-4_Units',\n        '5+ unitsunits': '5_plus_Units',\n        '_c12bldgs': '3-4_UnitBuilding',\n        '_c15bldgs': '5_plus_UnitBuilding',\n        '_c6bldgs': '1_UnitBuilding',\n        '_c9bldgs': '2_UnitBuilding',\n        'countyname': 'County',\n        'fips1state': 'StateFips',\n        'fips2county': 'CountyFips',\n        'surveydate': 'Date'\n    }\n\n    new_columns = [new_columns_names[_] for _ in building_permit_df.columns]\n    new_df = building_permit_df.toDF(*new_columns)\n    \n    # Create a date column from the shortened date\n    func = udf(lambda x: dt.datetime.strptime(x, \"%y%M%d\"), DateType())\n    fixed_date = new_df.withColumn('new_Date', to_date(col('Date'), \"yyyyMM\"))\n    #fixed_date = new_df.withColumn('new_Date', date_format(func(col('Date')), 'y-M'))\n    \n    month_dict = {\n    '01': 'Jan',\n    '02': 'Feb',\n    '03': 'Mar',\n    '04': 'Apr',\n    '05': 'May',\n    '06': 'Jun',\n    '07': 'Jul',\n    '08': 'Aug',\n    '09': 'Sep',\n    '10': 'Oct',\n    '11': 'Nov',\n    '12': 'Dec' \n    }\n    \n    # Split the year and month from the data and create new columns for those.\n    month_convert = udf(lambda x: month_dict[x])\n    year_month_split = fixed_date.withColumn('Year', split(fixed_date.new_Date,\"-\")[0]) \\\n                                 .withColumn('Month', split(fixed_date.new_Date,\"-\")[1])\n    \n    # Converting month to string month and then dropping the messy date\n    year_month_split = year_month_split.withColumn('Month', month_convert(col(\"Month\")))\n    cleaned_df = year_month_split.drop(\"Date\")\n    \n    # Rename Date Column, (and drop date again because it didn't fully drop the first time..)\n    fixed_date = cleaned_df.drop('Date')\n    fixed_date = fixed_date.withColumnRenamed(\"new_Date\", \"Date\")\n    \n    # Combine State and County Fips into single FIPS\n    other_columns = [_ for _ in fixed_date.columns if _ not in ['StateFips','CountyFips']]\n    finalized_frame = fixed_date.select(concat_ws('', fixed_date.StateFips, fixed_date.CountyFips).alias('FIPS'),*other_columns)\n\n    # Filter to only New Jersey records\n    finalized_frame = finalized_frame.where(col(\"FIPS\").like('34%'))\n    finalized_frame = finalized_frame.withColumn('County', trim(col('County')))\n    \n    # Replace nulls with zero and add units and buildings for overall.\n    reduced_df = finalized_frame.na.fill(0).withColumn(\"NewUnits\", col(\"1_Unit\") + col('2_Unit') + col('3-4_Units') + col('5_plus_Units'))\n    reduced_df = reduced_df.withColumn(\"NewBuildings\", col(\"3-4_UnitBuilding\") + col(\"5_plus_UnitBuilding\") + col(\"1_UnitBuilding\") + col(\"2_UnitBuilding\"))\n\n    # Drop Unneccessary columns.\n    cols_to_keep = ['FIPS', 'County', 'Year', 'Month', 'NewUnits', 'NewBuildings', 'Date']\n    cols_to_drop = [_ for _ in reduced_df.columns if _ not in cols_to_keep]\n    reduced_df = reduced_df.drop(*cols_to_drop)\n    return reduced_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8a07317-4147-41a6-9de8-5ba2fc9fac25"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Might remove this code block because we aren't adding to the database in this step any more**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0dd3e28-a25c-43f5-bb55-123f36e19dba"}}},{"cell_type":"code","source":["\n# def add_to_database(df):\n#     database = \"arctic_analysts_capstone\"\n#     table = \"dbo.building_permits\"\n#     user = \"arctic_analysts\"\n#     password  = \"ThisPassw0rd!\"\n#     server = \"gen10-data-fundamentals-22-02-sql-server.database.windows.net\"\n\n#     # WRITE <--- dataframe to database\n#     df.write.format(\"jdbc\") \\\n#       .option(\"url\", f\"jdbc:sqlserver://{server}:1433;databaseName={database};\") \\\n#       .mode(\"append\") \\\n#       .option(\"dbtable\", table) \\\n#       .option(\"user\", user) \\\n#       .option(\"password\", password) \\\n#       .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n#       .save()\n#     print('Successfully added the data to the dataframe.')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e09a6a04-f63f-4db1-a689-44fb1b500f0b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##Data Scrape Controller<br>\n\n> This cell controls whether to scrape or not, manages the results of the scrape, and saves any new links to the scrape history."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98973d9c-3a42-488e-a0f4-18e5ae1553fc"}}},{"cell_type":"code","source":["# If the scrape validator did find new data or needs to scrape.\nif new_data != 'none':\n    \n    print('Processing New Links')\n    # Scrape new data\n    response, checked_links = process_new_data()\n    \n    # Establish whether to overwrite or append new data\n    if new_data == 'all':\n        write_mode = 'overwrite'\n    else:\n        write_mode = 'append'\n    \n    # Create the checked link history\n    new_links_frame = spark.createDataFrame(checked_links, StringType())\n    new_links = new_links_frame.withColumnRenamed('value', 'InDatabase')\n    \n    print(\"Cleaning...\")\n    # Send the data to the cleaning function\n    cleansed = data_shower(response)\n\n    # Drop duplicates where date and location are the same\n    # This occurred because some files were duplicated\n    cleansed = cleansed.dropDuplicates(['Date','FIPS'])\n    \n    print(\"Saving...\")\n    # Send the data to the save function\n    save_new_data(cleansed)\n    ###################add_to_database(cleansed) Not adding to the database in this brick\n    \n    # Update the scrape history\n    update_links(new_links, write_mode)\nelse:\n    print('Data is up to date.')\nprint(\"Finished.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c37e0d7-42fe-487c-a207-8f1870605aa4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Data is up to date.\nFinished.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Data is up to date.\nFinished.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#building_permit_df = spark.read.options(inferSchema='True', header='True').json(read_path + \"/\" + getArgument('filename') + \".json\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1dfb8e7f-bb79-465a-8ced-43b24c8032b3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"arctic_capstone_building_permits_scraper","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{"input":{"nuid":"e126e506-c850-4684-aae3-b6d76c9f2c74","currentValue":"gen10datafund2202/z-arctic-analysts-capstone-main","widgetInfo":{"widgetType":"text","name":"input","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"filename":{"nuid":"8a0c493b-8a8b-4f4f-98db-54ad516cd10c","currentValue":"county_level_building_permits_from_databrick","widgetInfo":{"widgetType":"text","name":"filename","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"output":{"nuid":"fd7d9297-6b76-4b2c-8645-79aabd00d00d","currentValue":"gen10datafund2202/z-arctic-capstone-cleaned-data","widgetInfo":{"widgetType":"text","name":"output","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":4446153088461196}},"nbformat":4,"nbformat_minor":0}

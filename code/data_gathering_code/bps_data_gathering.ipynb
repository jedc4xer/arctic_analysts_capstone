{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22182bc0-3344-4082-abec-a49817a855d9",
   "metadata": {},
   "source": [
    "# Parsing Building Permit Data From US Census Data\n",
    "**Step 1:** Import Required Libraries\n",
    "> We will need:\n",
    "> 1. Pandas for data control\n",
    "> 2. Requests for getting the raw data\n",
    "> 3. Beautiful Soup for finding the links on the main summary page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919bc433-482d-41d0-a254-ca4f60aa2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae04c5-b5d8-4a84-8504-7397b77cafbf",
   "metadata": {},
   "source": [
    "**Step 2:** Establish the initial source and get the text from the page for parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabbc39d-bbc2-42ac-bc8f-8c0ecc906e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://www.census.gov/construction/bps/stateannual.html'\n",
    "source = 'https://www.census.gov'\n",
    "\n",
    "raw_html = requests.get(link).text\n",
    "soup = BeautifulSoup(raw_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ff6300-2617-4e19-bb7f-6512e3a6ab55",
   "metadata": {},
   "source": [
    "**Step 3:** Gather all the relevant links from the page and add them to a link list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0a24b-d340-44c2-bf81-8e9247fda57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('a')\n",
    "data_links = []\n",
    "for link in links:\n",
    "    try:\n",
    "        if (\".txt\" in link.get('href') and \"tb2u\" in link.get('href')) or (\".xls\" in link.get('href')):\n",
    "            data_links.append(source + link.get('href'))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1ad6e-cf7d-49e5-8264-5abc64ace340",
   "metadata": {},
   "source": [
    "**Step 4:** Define primary parsing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8d9e1-1dc4-40e8-a6a9-5b522d1e73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_excel(link):\n",
    "    \"\"\" \n",
    "    3 of the datasets are in .xlsx format and needed to be parsed specially.\n",
    "    \"\"\"\n",
    "    \n",
    "    bps_excel = pd.ExcelFile(link)\n",
    "    bps_excel = bps_excel.parse('State Units')\n",
    "\n",
    "    # Rename the columns for clarity\n",
    "    bps_excel.columns = [\n",
    "        'location','total','1 unit','2 units','3-4 units','5+ units','num_structures_with_5+'\n",
    "    ]\n",
    "    \n",
    "    # Drop the irrelevant rows at the top and\n",
    "    # drop any null rows.\n",
    "    bps_excel.drop([0,1,2,3,4,5], inplace = True)\n",
    "    bps_excel.dropna(inplace = True)\n",
    "    \n",
    "    # Add the year to the subset\n",
    "    bps_excel['year'] = link.replace(\"99.xls\",\"\")[-4:]\n",
    "    \n",
    "    return bps_excel\n",
    "\n",
    "def parse_text(link):\n",
    "    \"\"\" This is a more complicated function because\n",
    "        the returned data is in a non-uniform text file.\n",
    "        \"\"\"\n",
    "    \n",
    "    raw_page = requests.get(link).text\n",
    "\n",
    "    # Parse the text file\n",
    "    # There are many rows with no clear delimiter, so\n",
    "    # we are creating a delimiter and removing noise.\n",
    "    new_string = \"\"\n",
    "    space_count = 0\n",
    "    for char in raw_page:\n",
    "        if char == ' ':\n",
    "            space_count += 1\n",
    "            if space_count == 2:\n",
    "                new_string += \"|\"\n",
    "            else:\n",
    "                if space_count > 2:\n",
    "                    continue\n",
    "                else:\n",
    "                    new_string += char\n",
    "        else:\n",
    "            new_string += char\n",
    "            space_count = 0\n",
    "\n",
    "    # Split the newly created string and \n",
    "    # then split each row and add to a row list.\n",
    "    split_string = new_string.split(\"\\n\")\n",
    "    cleaned_rows = []\n",
    "    for row in split_string:\n",
    "        split_row = [_.strip() for _ in row.split(\"|\") if _ != \" \"]\n",
    "        if len(split_row) > 6:\n",
    "            cleaned_rows.append(split_row)\n",
    "\n",
    "    # Add column names and add the year.\n",
    "    bps_df = pd.DataFrame(cleaned_rows, columns = [\n",
    "        'location','total','1 unit','2 units','3-4 units','5+ units','num_structures_with_5+'\n",
    "    ])\n",
    "    \n",
    "    bps_df['year'] = link.replace(\".txt\",\"\").replace(\"_newuniv\",\"\")[-4:]\n",
    "    return bps_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc41c9-713d-43a0-9691-822a3fa428f8",
   "metadata": {},
   "source": [
    "**Step 5:** Get the data\n",
    "> Loop through the data links and determine how to parse the data.<br>Once the data is parsed, it is added to a primary dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586a9d36-5b82-4859-be68-6feabd7faad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = []\n",
    "bps_main = pd.DataFrame()\n",
    "for link in data_links:\n",
    "    \n",
    "    # There is one year that was mostly duplicated, so we are only using the \n",
    "    # more complete data.\n",
    "    if link.replace(\".txt\",\"\").replace(\"_newuniv\",\"\")[-4:] in years:\n",
    "        print(f'{link} is not needed.')\n",
    "        continue\n",
    "        \n",
    "    # Determine whether to parse text or xlsx\n",
    "    if \".txt\" not in link:\n",
    "        bps_df = parse_excel(link)\n",
    "    else:\n",
    "        bps_df = parse_text(link)\n",
    "    \n",
    "    # We are saving the years to know when we can skip a year (see above).\n",
    "    years.append(bps_df.year.tolist()[0])\n",
    "    \n",
    "    # Add the gathered data to the master dataframe.\n",
    "    bps_main = pd.concat([bps_main, bps_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9202bf-a8c1-4134-a726-775402342ee5",
   "metadata": {},
   "source": [
    "**Step 6:** Sort the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92252aee-8558-45b1-905c-5259a861d6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bps_main.sort_values(by = ['year','location'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dceb472-f67b-43a3-be6f-c31645d3cb9e",
   "metadata": {},
   "source": [
    "**Step 7:** Preserve the data for further cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac68071b-472d-4a3b-8a84-60f2f14b3141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A subfolder named \"data\" is required to store the gathered data.\n",
    "bps_main.to_csv(\n",
    "    'data/newly_authorized_privately_owned_housing_units.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
